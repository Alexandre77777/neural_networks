{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexandre77777/neural_networks/blob/main/11.%20%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE%20%D1%8F%D0%B7%D1%8B%D0%BA%D0%B0.%20%D0%91%D0%BE%D0%BB%D1%8C%D1%88%D0%B8%D0%B5%20%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D1%8B%D0%B5%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%20(LLM)/%D0%A2%D0%B5%D0%BC%D0%B0_%E2%84%9612_%D0%A1%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5_%D1%87%D0%B0%D1%82_%D0%B1%D0%BE%D1%82%D0%B0_%D1%81_RAG_%D0%BD%D0%B0_%D0%B1%D0%B0%D0%B7%D0%B5_%D0%BB%D0%BE%D0%BA%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B9_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f717f783",
      "metadata": {
        "id": "f717f783"
      },
      "source": [
        "## **Тема №12. Создание интеллектуальной системы на основе больших языковых моделей (LLM) с применением технологии Retrieval Augmented Generation (RAG)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Введение**\n"
      ],
      "metadata": {
        "id": "DF3Xvpee_X_G"
      },
      "id": "DF3Xvpee_X_G"
    },
    {
      "cell_type": "markdown",
      "id": "cb2af7be",
      "metadata": {
        "id": "cb2af7be"
      },
      "source": [
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Этот блокнот посвящен практической реализации системы типа \"вопрос-ответ\" (Question Answering, QA), основанной на взаимодействии Большой Языковой Модели (LLM) с внешней базой знаний, представленной PDF-документом.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Мы рассмотрим архитектурный подход **Retrieval-Augmented Generation (RAG)**, или **Генерация с Дополненной Выборкой**.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Этот метод позволяет динамически обогащать контекст, подаваемый на вход LLM, релевантной информацией перед тем, как модель сгенерирует ответ.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Такой подход является основным для создания систем, способных оперировать актуальными или специфическими данными, которые не входили в исходный набор данных для обучения LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Установка и импорт необходимых пакетов**\n"
      ],
      "metadata": {
        "id": "8_7AMkrV_n7C"
      },
      "id": "8_7AMkrV_n7C"
    },
    {
      "cell_type": "markdown",
      "id": "6953edeb",
      "metadata": {
        "id": "6953edeb"
      },
      "source": [
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Современные системы искусственного интеллекта, особенно в области обработки естественного языка (Natural Language Processing, NLP), строятся с использованием специализированных программных библиотек и фреймворков.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Эти инструменты предоставляют удобные средства для взаимодействия с моделями, обработки данных и построения сложных конвейеров обработки (pipelines). В нашем проекте мы будем использовать:\n",
        "*   **`LangChain`**: Фреймворк для оркестрации компонентов приложений на базе LLM, упрощающий управление промптами, интеграцию с источниками данных и моделями.\n",
        "*   **`Ollama`**: Среда для локального развертывания и запуска LLM, позволяющая работать с моделями без обращения к облачным сервисам.\n",
        "*   **`ChromaDB`**: Векторная база данных, оптимизированная для задач поиска по семантической близости – ключевой компонент для RAG.\n",
        "*   **`Gradio`**: Библиотека для быстрого создания интерактивных веб-интерфейсов, удобная для демонстрации работы модели.\n",
        "*   **`PyMuPDF`**: Библиотека для извлечения текста из PDF-документов.\n",
        "\n",
        "Для начала установим и импортируем эти компоненты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "87bbb523",
      "metadata": {
        "id": "87bbb523"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ollama\n",
        "!pip install langchain chromadb gradio\n",
        "!pip install -U langchain-community\n",
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Установка Ollama в Google Colab\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install ollama-python pyngrok"
      ],
      "metadata": {
        "id": "wiwDmRFl_2KL"
      },
      "id": "wiwDmRFl_2KL",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "452bc737",
      "metadata": {
        "id": "452bc737"
      },
      "outputs": [],
      "source": [
        "import ollama  # Для взаимодействия с локальными LLM через Ollama\n",
        "import gradio as gr  # Для создания веб-интерфейса\n",
        "import os      # Для работы с файловой системой (проверка файлов, путей)\n",
        "import re      # Для работы с регулярными выражениями (очистка ответа модели)\n",
        "\n",
        "# Компоненты LangChain для обработки документов и RAG\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader, DirectoryLoader # Загрузчики для PDF, TXT и директорий\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Для разбиения текста на чанки\n",
        "from langchain_community.vectorstores import Chroma  # Векторная база данных ChromaDB\n",
        "from langchain_community.embeddings import OllamaEmbeddings  # Модель для создания эмбеддингов через Ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc49b5b9",
      "metadata": {
        "id": "bc49b5b9"
      },
      "source": [
        "*   **Пояснение:**\n",
        "    *   Мы устанавливаем: `ollama` (для запуска LLM локально), `langchain` (фреймворк для LLM-приложений), `chromadb` (векторная база данных), `gradio` (для веб-интерфейса), `pymupdf` (для чтения PDF) и `langchain-community` (дополнительные компоненты LangChain).\n",
        "    *   Затем мы импортируем необходимые классы и функции:\n",
        "        *   `ollama`: Для прямого взаимодействия с LLM.\n",
        "        *   `gradio`: Для создания пользовательского интерфейса.\n",
        "        *   `re`: Для работы с регулярными выражениями (очистка текста).\n",
        "        *   `PyMuPDFLoader`: Загрузчик текста из PDF (из `langchain_community`).\n",
        "        *   `RecursiveCharacterTextSplitter`: Инструмент для разбиения текста на части (из `langchain`).\n",
        "        *   `Chroma`: Векторная база данных (из `langchain`).\n",
        "        *   `OllamaEmbeddings`: Модель для создания векторных представлений текста (из `langchain_community`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Базовое Взаимодействие с LLM (без RAG)**\n"
      ],
      "metadata": {
        "id": "B9Zf4ILwADbV"
      },
      "id": "B9Zf4ILwADbV"
    },
    {
      "cell_type": "markdown",
      "id": "89f74823",
      "metadata": {
        "id": "89f74823"
      },
      "source": [
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;В основе современных достижений в обработке естественного языка лежат **большие языковые модели (LLM)**. Как правило, это глубокие нейронные сети, построенные на архитектуре **Трансформер (Transformer)**.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Их обучение происходит на гигантских текстовых корпусах с использованием задач самообучения (self-supervised learning), например, предсказания следующего слова. В результате модель накапливает обширные **параметрические знания** о языке, фактах и закономерностях мира, которые кодируются в ее весах.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Способность LLM решать разнообразные задачи часто рассматривается как **эмерджентное свойство**, проявляющееся при достижении моделью определенного масштаба.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Прежде чем строить RAG-систему, посмотрим, как можно напрямую взаимодействовать с локально развернутой LLM с помощью `Ollama`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Запуск Ollama в фоновом режиме\n",
        "!nohup ollama serve &"
      ],
      "metadata": {
        "id": "i9RQfzS-1vfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b2b26a-339f-4c9f-cc52-f0680d86883e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ],
      "id": "i9RQfzS-1vfK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка моделей в Ollama\n",
        "%%capture\n",
        "!ollama pull deepseek-r1:1.5b\n",
        "!ollama pull deepseek-r1:8b\n",
        "!ollama pull llama3.1"
      ],
      "metadata": {
        "id": "nl_SEEWd3sby"
      },
      "execution_count": 6,
      "outputs": [],
      "id": "nl_SEEWd3sby"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Тестируем модель \"deepseek-r1:1.5b\":**"
      ],
      "metadata": {
        "id": "SZOdAHQz9NG3"
      },
      "id": "SZOdAHQz9NG3"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"deepseek-r1:1.5b\""
      ],
      "metadata": {
        "id": "gkW20XlVEHc0"
      },
      "id": "gkW20XlVEHc0",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Прямой вызов LLM для ответа на вопрос\n",
        "llm_response = ollama.chat(\n",
        "    model=MODEL, # Указываем используемую модель\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Объясни второй закон Ньютона на профессиональном русском языке.\"}, # Запрос пользователя\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Вывод ответа модели\n",
        "print(llm_response[\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "782StRVNJAqy",
        "outputId": "1a7f2ffc-993c-42c3-f3a5-ba66e06c31f0"
      },
      "id": "782StRVNJAqy",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Хм, пользователь просит объяснить Second Law of Newton на профессиональном русском языке. Я помню, что это законы Письма, которые важны в физике и индустрии. Давайте посмотрим.\n",
            "\n",
            "Сначала мне нужно понять, о чем Second Law. Оно связано с изменением Momentum. Мне кажется, это means change in momentum is equal to force applied. Но как именно он выглядит на практике?\n",
            "\n",
            "Поскольку пользователь использует русский язык, я должен его использовать в правильном контексте. Возможно, стоит упомянуть различные темы применения Second Law: для споделывания движения тела и центральной тяжелистости.\n",
            "\n",
            "Но как лучше выразить это? Может быть, подчеркнуть, что Second Law описывает relationship between force and acceleration, а также, как это может изменять.velocity и позицию.\n",
            "\n",
            "Также стоит упомянуть, что Second Law не утверждает因果ية, только прямые связи между magnitudes. Это важно для понимания, где можно применить его.\n",
            "\n",
            "Проверю, есть ли дополнительные моменты, которые могут быть важными: например, связь с кинематикой и гидrodинамикой, или физикой течения и воли. Но для профессионального подхода, возможно, стоит более глубокое описание.\n",
            "\n",
            "Итак, структурирую ответ: начать с общего понятия Second Law, его математического выражение, применение на практике (споделывание движения тела и центральной тяжелистости), Mention of other applications, и заключение о важности для индустрии.\n",
            "</think>\n",
            "\n",
            "**Введение**\n",
            "\n",
            "Среди физических законов есть Second Law, который playulates ключевую роль в понимании взаимосвязей между силой и ускорением. Это законы Ньютона, которые помогают описывать изменение Momentum и позиции тела.\n",
            "\n",
            "**Первый закон**\n",
            "\n",
            "В первом из законов Ньютона гласится, что каждый тело не自发ично движется. Если он обретает speed (ускорение), то необходимо устить FORCE. Это означает, что без силы, тело не будет изменять своей скорости.\n",
            "\n",
            "**Следующий закон**\n",
            "\n",
            "В втором законе гласится: \"Каждый речь идет о力量е — это change in momentum\". Если Momentum (мoment) утешается или вовлечена других физических величинами, то необходимо применить сила. Это указывает на прямую связь между силой и изменениями Momentum.\n",
            "\n",
            "**Применение Second Law**\n",
            "\n",
            "В practice Second Law используется для описывания motion (мovers) и rotation (вращения). Например:\n",
            "\n",
            "- **Споделывание движения тела**: Если мавер (momentum) утешается, то необходимо приносить силу.\n",
            "- **Звёзды и центральная тяжелистость**: В физике центральная тяжелистость описывается как Momentum в центральном центрах.\n",
            "\n",
            "**Дополнительные применения**\n",
            "\n",
            "В practice Second Law также используется для гидrodинамiki, газов и физики течения. Например:\n",
            "\n",
            "- **Кинематика**: Изучение движения тел.\n",
            "- **Гидрудина']: Вiscidi: описывание фаз ветра и упругости.\n",
            "\n",
            "**Итог**\n",
            "\n",
            "В summary, Second Law Ньютона описывает relationship between force and acceleration, change in momentum. Это ключевой закон, который играет важную роль в practice mechanics и physics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Тестируем модель \"deepseek-r1:8b\":**"
      ],
      "metadata": {
        "id": "yqG_wujW9dx1"
      },
      "id": "yqG_wujW9dx1"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"deepseek-r1:8b\""
      ],
      "metadata": {
        "id": "Yy51Kjtc95ag"
      },
      "id": "Yy51Kjtc95ag",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Прямой вызов LLM для ответа на вопрос\n",
        "llm_response = ollama.chat(\n",
        "    model=MODEL, # Указываем используемую модель\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Объясни второй закон Ньютона на профессиональном русском языке.\"}, # Запрос пользователя\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Вывод ответа модели\n",
        "print(llm_response[\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwGonQpu97lX",
        "outputId": "3df1d876-99a5-49ba-e95d-e83d8fc6cd1e"
      },
      "id": "RwGonQpu97lX",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Хорошо, я пытаюсь разобраться с вторым законом Ньютона. Я помню, что законы Ньютона связаны с运动ом и гравитацией, но я не уверен, как именно они работают. Второй закон, возможно, связан с определением момента импульса или изменения движения тела.\n",
            "\n",
            "Первое, что приходит в голову, это то, что первый закон Ньютона о том, что силовые矢量 количества массы размягчености равны impulse of change of momentum. То есть F = dp/dt. Это я понял, но как第二ый закон связан с этим?\n",
            "\n",
            "Пользователь спрашивает на русском языке, так что мне нужно убедиться, что я правильно понимаю термины и объяснения.\n",
            "\n",
            "Далее, я пытаюсь найти определение второго закона Ньютона. Возможно, он имеет два аспекта: первый касается того, как力 создает изменение момента импульса, а второй — о сохранении количества движения в изолированной системе.\n",
            "\n",
            "Но я не уверен, правильно ли я это понимаю. Может быть, мне нужно рассмотреть примеры или иллюстрации, чтобы лучше понять.\n",
            "\n",
            "Если бы я хотел объяснить это на профессиональном русском языке, я должен использовать точные термины и структурировать информацию.\n",
            "\n",
            "Также, возможно, стоит упомянуть приложения второго закона в разных областях, например, в астронимии или механиках.\n",
            "\n",
            "В итоге, мне нужно четко изложить, как второй закон Ньютона объясняет создание момента импульса и его сохранение в изолированных системах.\n",
            "</think>\n",
            "\n",
            "Второй закон Ньютона — это закон классической механики, который описывает relationship between force and momentum.\n",
            "\n",
            "### Определение:\n",
            "\n",
            "**Второй закон Ньютона** states that the impulse (force multiplied by time) is equal to the change in momentum. Mathematically, this is expressed as:\n",
            "\\[ F = \\frac{dp}{dt} \\]\n",
            "где \\( F \\) — force, \\( p \\) — momentum, и \\( t \\) — время.\n",
            "\n",
            "### Применение:\n",
            "\n",
            "1. **Создание момента импульса:** Закон объясняет, как力 создает изменение движения тела. Например, если сила applies to an object, она вызывает изменение её скорости, что влечет за собой изменение момента импульса.\n",
            "   \n",
            "2. **Сохранение количества движения в изолированной системе:** В изолированной системе количество движения (моментум) остаётся неизменным. Это важно для понимания таких процессов, как столкновение объектов или运动 в невесомой среде.\n",
            "\n",
            "### Примеры применения:\n",
            "\n",
            "- **Астрономия:** Закон используется для расчёта движения планет и卫илей.\n",
            "- **Механика:** Важен при расчетах столкновений и коллизий, а также в динамических системах.\n",
            "\n",
            "### Заключение:\n",
            "\n",
            "Второй закон Ньютона объясняет, как力 создает изменение импульса и как количество движения сохраняется в изолированных системах. Это основа многих процессов в физике и инженерии.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Тестируем модель \"llama3.1\":**"
      ],
      "metadata": {
        "id": "KZRVDrMh9iik"
      },
      "id": "KZRVDrMh9iik"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"llama3.1\""
      ],
      "metadata": {
        "id": "DSXGXDNn-KD7"
      },
      "execution_count": 11,
      "outputs": [],
      "id": "DSXGXDNn-KD7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Прямой вызов LLM для ответа на вопрос\n",
        "llm_response = ollama.chat(\n",
        "    model=MODEL, # Указываем используемую модель\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Объясни второй закон Ньютона на профессиональном русском языке.\"}, # Запрос пользователя\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Вывод ответа модели\n",
        "print(llm_response[\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXKqFNzf99wu",
        "outputId": "843e00b8-1819-433b-ca31-fbf7e82ca8a4"
      },
      "id": "JXKqFNzf99wu",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Второй закон Ньютона гласит, что при действии внешней силы на предмет у него появляется и ускорение пропорциональное величине силы и обратно пропорциональное его массе.\n",
            "\n",
            "Формально он можно выразить следующим образом:\n",
            "\n",
            "F = мa,\n",
            "\n",
            "где F — сила, а m — масса объекта. Аскорбиновое число (a) — это разность между ускорением предмета при наложении силы и его ускорением в отсутствии сил.\n",
            "\n",
            "Также второй закон Ньютона можно выразить как:\n",
            "\n",
            "F = м(Δv/Δt),\n",
            "\n",
            "где Δv — изменение скорости объекта, а Δt — время изменения скорости. \n",
            "\n",
            "Этот закон имеет важное значение для понимания движения предметов и их взаимодействия с окружающей средой.\n",
            "\n",
            "Природа закона заключается в том, что сила, действующая на систему, влияет на ее ускорение, и величина этого ускорения зависит от массы системы.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc0422ff",
      "metadata": {
        "id": "cc0422ff"
      },
      "source": [
        "*   **Пояснение:**\n",
        "    *   **Большая Языковая Модель (LLM - Large Language Model):** Это нейронная сеть, обученная на огромных объемах текста, способная понимать и генерировать человекоподобный ответ.\n",
        "    *   Функция `ollama.chat()` отправляет запрос (`messages`) к указанной модели (`model`).\n",
        "    *   Ответ модели содержится в словаре `llm_response` по ключу `[\"message\"][\"content\"]`.\n",
        "    *   На данном этапе ответ генерируется исключительно на основе внутренних, параметрических знаний модели, без учета каких-либо внешних документов."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Предварительная Обработка PDF-документа для RAG**\n"
      ],
      "metadata": {
        "id": "4_pFZATlA1M7"
      },
      "id": "4_pFZATlA1M7"
    },
    {
      "cell_type": "markdown",
      "id": "c47c0d3f",
      "metadata": {
        "id": "c47c0d3f"
      },
      "source": [
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Чтобы LLM могла использовать информацию из внешних источников, таких как наши PDF-документы, в RAG-системах применяется механизм **информационного поиска (Information Retrieval, IR)**.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Современный подход к поиску в текстовых данных основан на **векторных представлениях (embeddings)**. Текст (слова, предложения, абзацы) преобразуется в числовые векторы в многомерном **латентном пространстве** таким образом, что семантически близкие фрагменты текста оказываются рядом в этом пространстве (например, по косинусному расстоянию).\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Для хранения таких векторов и быстрого поиска по сходству используются **векторные базы данных**. Они применяют эффективные алгоритмы **приближенного поиска ближайших соседей (Approximate Nearest Neighbors, ANN)**, например, на основе графовых (HNSW) или кластерных (IVF) индексов. Перед созданием векторов исходный документ обычно разбивают на **чанки (chunks)** – небольшие фрагменты.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Это необходимо, во-первых, из-за ограничения на длину входного текста, который LLM может обработать за раз (**контекстное окно**), а во-вторых, для повышения точности поиска, позволяя находить более гранулярные и релевантные части документа.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Подготовим функцию, которая выполнит все эти шаги для нашего PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "58853c22",
      "metadata": {
        "id": "58853c22"
      },
      "outputs": [],
      "source": [
        "# Функция обработки PDF: загрузка, разбиение, эмбеддинг, сохранение в ChromaDB\n",
        "def process_pdf_for_rag(pdf_filepath):\n",
        "    \"\"\"Загружает PDF, разбивает на чанки, создает эмбеддинги и ретривер.\"\"\"\n",
        "    if pdf_filepath is None:\n",
        "        print(\"PDF-файл не предоставлен.\")\n",
        "        return None, None # Возвращаем None для vector_database и document_retriever\n",
        "\n",
        "    # 1. Загрузка текста из PDF\n",
        "    loader = PyMuPDFLoader(pdf_filepath)\n",
        "    loaded_documents = loader.load()\n",
        "    if not loaded_documents:\n",
        "        print(\"Не удалось извлечь текст из PDF.\")\n",
        "        return None, None\n",
        "\n",
        "    # 2. Разбиение текста на чанки\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    document_chunks = text_splitter.split_documents(loaded_documents)\n",
        "\n",
        "    # 3. Создание модели эмбеддингов\n",
        "    embedding_model = OllamaEmbeddings(model=MODEL)\n",
        "\n",
        "    # 4. Создание и наполнение векторной базы данных\n",
        "    # persist_directory - папка для сохранения БД на диске\n",
        "    vector_database = Chroma.from_documents(\n",
        "        documents=document_chunks,\n",
        "        embedding=embedding_model,\n",
        "        persist_directory=\"chroma_db_rag_concise/\"\n",
        "    )\n",
        "\n",
        "    # 5. Создание ретривера\n",
        "    document_retriever = vector_database.as_retriever()\n",
        "\n",
        "    print(f\"PDF обработан: {len(document_chunks)} чанков сохранено в ChromaDB.\")\n",
        "    return vector_database, document_retriever # Возвращаем только то, что нужно дальше"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b0aa6df",
      "metadata": {
        "id": "1b0aa6df"
      },
      "source": [
        "*   **Пояснение:**\n",
        "    *   Функция `process_pdf_for_rag` принимает путь к PDF-файлу (`pdf_filepath`).\n",
        "    *   **Шаг 1: Загрузка:** `PyMuPDFLoader` извлекает текст из PDF.\n",
        "    *   **Шаг 2: Разбиение:** `RecursiveCharacterTextSplitter` делит текст на **чанки (chunks)** - небольшие фрагменты (здесь по 500 символов с перекрытием 100 символов). Это необходимо из-за ограничения на объем текста, который LLM может обработать за раз (контекстное окно), а перекрытие помогает сохранить связи между чанками.\n",
        "    *   **Шаг 3: Эмбеддинги:** `OllamaEmbeddings` инициализирует модель для преобразования текста в **векторные представления (embeddings)**. Эмбеддинги - это числовые векторы, отражающие семантическое значение текста. Похожие по смыслу тексты имеют близкие векторы.\n",
        "    *   **Шаг 4: Векторная БД:** `Chroma.from_documents` создает **векторную базу данных (`vector_database`)**. Она сохраняет чанки и их эмбеддинги, позволяя эффективно искать похожие по смыслу фрагменты. `persist_directory` указывает, где хранить базу данных локально.\n",
        "    *   **Шаг 5: Ретривер:** `vector_database.as_retriever()` создает **ретривер (`document_retriever`)**. Это объект, который будет выполнять поиск наиболее релевантных чанков в базе данных по запросу пользователя.\n",
        "    *   Функция возвращает созданную базу данных и ретривер."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Объединение найденных фрагментов документа**\n"
      ],
      "metadata": {
        "id": "XBMIsembBF1n"
      },
      "id": "XBMIsembBF1n"
    },
    {
      "cell_type": "markdown",
      "id": "984198b4",
      "metadata": {
        "id": "984198b4"
      },
      "source": [
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;На этапе поиска (retrieval) система извлекает из базы знаний набор фрагментов (чанков), наиболее релевантных запросу пользователя.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Поскольку LLM обрабатывает входные данные как единую последовательность, эти фрагменты нужно объединить в связный текст. Этот объединенный контекст затем добавляется к исходному запросу.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Качество и структура этого объединенного текста напрямую влияют на способность модели эффективно использовать предоставленную информацию.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Важно также следить, чтобы итоговый текст не превышал максимальную длину **контекстного окна (context window)** модели. Создадим простую функцию для такого объединения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4b6ee322",
      "metadata": {
        "id": "4b6ee322"
      },
      "outputs": [],
      "source": [
        "# Функция для объединения текстов из списка документов LangChain\n",
        "def combine_retrieved_documents(retrieved_documents):\n",
        "    \"\"\"Объединяет page_content найденных документов в одну строку.\"\"\"\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5623ac",
      "metadata": {
        "id": "9b5623ac"
      },
      "source": [
        "*   **Пояснение:**\n",
        "    *   Ретривер возвращает список документов (чанков). Эта функция извлекает текстовое содержимое (`page_content`) каждого документа и соединяет их через двойной перенос строки (`\\n\\n`), формируя единый контекст для LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Запрос к LLM с Дополненным Контекстом**\n"
      ],
      "metadata": {
        "id": "3GPrsvm2BR39"
      },
      "id": "3GPrsvm2BR39"
    },
    {
      "cell_type": "markdown",
      "id": "95f4d075",
      "metadata": {
        "id": "95f4d075"
      },
      "source": [
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Правильное формирование входных данных для LLM, известное как **инженерия промптов (prompt engineering)**, играет ключевую роль в получении качественных результатов. В RAG-системах промпт обычно строится так, чтобы четко разделить исходный вопрос пользователя и извлеченный из документов контекст. Это помогает модели понять, что ответ следует генерировать, опираясь на предоставленную информацию.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Такой механизм можно рассматривать как форму **обучения в контексте (in-context learning)**, когда модель адаптирует свое поведение на основе инструкций или примеров, данных во входном промпте, без изменения своих весов. Явное указание ролей (\"Вопрос:\", \"Контекст:\") направляет модель на решение конкретной задачи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9b620624",
      "metadata": {
        "id": "9b620624"
      },
      "outputs": [],
      "source": [
        "# Функция для запроса к LLM с использованием контекста\n",
        "def query_llm_with_context(user_question, retrieved_context):\n",
        "    \"\"\"Формирует промпт и вызывает LLM, передавая вопрос и контекст.\"\"\"\n",
        "\n",
        "    # Формирование промпта для LLM\n",
        "    llm_prompt = f\"Question: {user_question}\\n\\nContext: {retrieved_context}\\n\\nAnswer:\"\n",
        "\n",
        "    # Вызов LLM\n",
        "    response = ollama.chat(\n",
        "        model=MODEL,\n",
        "        messages=[{'role': 'user', 'content': llm_prompt}]\n",
        "    )\n",
        "\n",
        "    # Извлечение и очистка ответа\n",
        "    raw_response_content = response['message']['content']\n",
        "    # Удаление тегов <think>...</think>, если они есть\n",
        "    cleaned_response = re.sub(r'<think>.*?</think>', '', raw_response_content, flags=re.DOTALL).strip()\n",
        "\n",
        "    return cleaned_response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bee7152",
      "metadata": {
        "id": "2bee7152"
      },
      "source": [
        "*   **Пояснение:**\n",
        "    *   **Промпт (Prompt):** Это структурированная инструкция или запрос, подаваемый LLM. Здесь `llm_prompt` содержит и вопрос пользователя (`user_question`), и найденный контекст (`retrieved_context`), чтобы модель генерировала ответ на основе предоставленной информации.\n",
        "    *   Функция отправляет промпт модели `deepseek-r1:1.5b`.\n",
        "    *   Ответ извлекается и очищается от возможных вспомогательных тегов (например, `<think>...</think>`, которые модель использует для \"рассуждений\") с помощью регулярного выражения."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Построение RAG-пайплайна**\n"
      ],
      "metadata": {
        "id": "xplJ8YfmBl0V"
      },
      "id": "xplJ8YfmBl0V"
    },
    {
      "cell_type": "markdown",
      "id": "6596cfbd",
      "metadata": {
        "id": "6596cfbd"
      },
      "source": [
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Подход **Retrieval-Augmented Generation (RAG)** сочетает **параметрические знания**, хранящиеся в весах LLM, с **непараметрическими знаниями**, которые динамически извлекаются из внешнего источника данных во время работы модели (**инференса**).\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;В отличие от дообучения (fine-tuning), RAG позволяет использовать актуальную или специфическую информацию без необходимости дорогостоящего переобучения LLM.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Типичный RAG-пайплайн включает шаги: получение запроса, поиск релевантных документов в базе знаний, дополнение запроса найденной информацией и генерация ответа LLM на основе этого дополненного запроса.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Фреймворки, такие как `LangChain`, предлагают удобные средства для построения и запуска таких многошаговых процессов. Соберем все наши компоненты в единую RAG-цепочку."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция, реализующая RAG-пайплайн\n",
        "def rag_chain(user_question, document_retriever):\n",
        "    \"\"\"Выполняет поиск релевантных документов и генерирует ответ LLM с их учетом.\"\"\"\n",
        "\n",
        "    # 1. Поиск релевантных документов (Retrieval)\n",
        "    relevant_documents = document_retriever.invoke(user_question)\n",
        "\n",
        "    # 2. Форматирование контекста\n",
        "    combined_context = combine_retrieved_documents(relevant_documents)\n",
        "\n",
        "    # 3. Генерация ответа с контекстом (Generation)\n",
        "    final_answer = query_llm_with_context(user_question, combined_context)\n",
        "\n",
        "    return final_answer"
      ],
      "metadata": {
        "id": "g900cVbWB6mi"
      },
      "id": "g900cVbWB6mi",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e14c81b1",
      "metadata": {
        "id": "e14c81b1"
      },
      "outputs": [],
      "source": [
        "# Функция-обертка для Gradio интерфейса\n",
        "def answer_question_from_pdf(pdf_file_object, user_question):\n",
        "    \"\"\"Обрабатывает PDF (если есть) и отвечает на вопрос с помощью RAG.\"\"\"\n",
        "    vector_database = None\n",
        "    document_retriever = None\n",
        "\n",
        "    if pdf_file_object is not None:\n",
        "        pdf_filepath = pdf_file_object.name # Получаем путь к временному файлу Gradio\n",
        "        print(f\"Обработка файла: {pdf_filepath}\")\n",
        "        try:\n",
        "            # Обрабатываем PDF и получаем ретривер\n",
        "            vector_database, document_retriever = process_pdf_for_rag(pdf_filepath)\n",
        "            if document_retriever is None:\n",
        "                 return \"Ошибка: Не удалось обработать PDF.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке PDF: {e}\")\n",
        "            return f\"Ошибка при обработке PDF: {e}. Убедитесь, что файл корректен.\"\n",
        "    else:\n",
        "        # Если PDF не загружен, RAG невозможен для этого запроса\n",
        "         return \"Пожалуйста, загрузите PDF-файл, чтобы задать вопрос по его содержимому.\"\n",
        "\n",
        "    if not user_question:\n",
        "        return \"Пожалуйста, введите вопрос.\"\n",
        "\n",
        "    # Если PDF обработан успешно, выполняем RAG\n",
        "    print(f\"Выполнение RAG для вопроса: '{user_question}'\")\n",
        "    try:\n",
        "        result = rag_chain(user_question, document_retriever)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при выполнении RAG: {e}\")\n",
        "        return f\"Ошибка при генерации ответа: {e}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a88ec655",
      "metadata": {
        "id": "a88ec655"
      },
      "source": [
        "*   **Пояснение:**\n",
        "    *   **RAG-пайплайн (RAG Pipeline/Chain):** Это последовательность действий: поиск релевантной информации -> дополнение запроса этой информацией -> генерация ответа LLM.\n",
        "    *   Функция `rag_chain` реализует этот пайплайн:\n",
        "        1.  `document_retriever.invoke(user_question)`: Использует ретривер для поиска в векторной базе данных чанков, наиболее релевантных вопросу пользователя.\n",
        "        2.  `combine_retrieved_documents`: Объединяет найденные чанки в единый контекст.\n",
        "        3.  `query_llm_with_context`: Вызывает LLM с вопросом и сформированным контекстом.\n",
        "    *   Функция `answer_question_from_pdf` является основной логикой для Gradio:\n",
        "        *   Она принимает объект файла (`pdf_file_object`) и вопрос (`user_question`).\n",
        "        *   Если файл загружен, она извлекает путь к нему (`pdf_file_object.name`) и вызывает `process_pdf_for_rag` для создания/загрузки векторной базы и ретривера. Добавлена обработка ошибок на случай проблем с PDF.\n",
        "        *   Если файл не загружен или произошла ошибка при его обработке, RAG не выполняется.\n",
        "        *   Если ретривер успешно создан и вопрос задан, вызывается `rag_chain` для получения ответа. Добавлена обработка ошибок на случай проблем при выполнении RAG.\n",
        "        *   Возвращает сгенерированный ответ или сообщение об ошибке."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Создание Интерфейса Чат-бота с Помощью Gradio**\n"
      ],
      "metadata": {
        "id": "3wELDT71CEBh"
      },
      "id": "3wELDT71CEBh"
    },
    {
      "cell_type": "markdown",
      "id": "0c41a5e9",
      "metadata": {
        "id": "0c41a5e9"
      },
      "source": [
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Важной частью разработки прикладных систем ИИ является создание удобных средств взаимодействия с пользователем (**человеко-машинное взаимодействие, Human-Computer Interaction, HCI**).\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Инструменты вроде `Gradio` позволяют быстро создавать интерактивные веб-интерфейсы для демонстрации работы моделей машинного обучения, проведения экспериментов и сбора обратной связи. Предоставление простого интерфейса для таких систем, как наш RAG-пайплайн, делает технологию доступнее и упрощает ее тестирование. Создадим такой интерфейс."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b7b53e2f",
      "metadata": {
        "id": "b7b53e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "outputId": "75d3f799-4d0f-4578-a676-b7298417fd5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Запуск интерфейса Gradio...\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2c09e7bb15ddf5a4da.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2c09e7bb15ddf5a4da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Создание и запуск интерфейса Gradio\n",
        "chatbot_interface = gr.Interface(\n",
        "    fn=answer_question_from_pdf, # Функция, обрабатывающая ввод/вывод\n",
        "    inputs=[\n",
        "        gr.File(label=\"Загрузите PDF\", file_types=['.pdf']), # Поле для загрузки PDF\n",
        "        gr.Textbox(label=\"Задайте вопрос по документу\")    # Поле для ввода вопроса\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Ответ\"), # Поле для вывода ответа\n",
        "    title=f\"Чат-бот для PDF (RAG + {MODEL})\",\n",
        "    description=\"Загрузите PDF и задайте вопрос. Модель ответит, используя содержимое документа.\",\n",
        "    allow_flagging='never'\n",
        ")\n",
        "\n",
        "# Запуск интерфейса\n",
        "print(\"Запуск интерфейса Gradio...\")\n",
        "chatbot_interface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf39057",
      "metadata": {
        "id": "1cf39057"
      },
      "source": [
        "*   **Пояснение:**\n",
        "    *   `gr.Interface` создает веб-приложение.\n",
        "    *   `fn`: Основная функция (`answer_question_from_pdf`), которая будет вызываться.\n",
        "    *   `inputs`: Список компонентов для ввода данных (загрузка файла `gr.File` и текстовое поле `gr.Textbox`). `file_types=['.pdf']` ограничивает тип загружаемых файлов.\n",
        "    *   `outputs`: Компонент для вывода результата (`gr.Textbox`).\n",
        "    *   `title`, `description`: Заголовок и описание интерфейса.\n",
        "    *   `chatbot_interface.launch()`: Запускает локальный веб-сервер, делая интерфейс доступным в браузере."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Удаление Установленных Компонентов (Опционально)**\n"
      ],
      "metadata": {
        "id": "vU7PXBGRCJZI"
      },
      "id": "vU7PXBGRCJZI"
    },
    {
      "cell_type": "markdown",
      "id": "f3f794e6",
      "metadata": {
        "id": "f3f794e6"
      },
      "source": [
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;В процессе разработки важно уметь управлять установленными программными компонентами и созданными артефактами (моделями, базами данных). Корректное удаление неиспользуемых элементов освобождает системные ресурсы и помогает поддерживать порядок в рабочей среде. Ниже приведены инструкции по удалению основных компонентов, которые мы использовали.\n",
        "\n",
        "*   **Удаление Ollama:**\n",
        "    *   macOS: Удалите приложение Ollama из папки \"Программы\".\n",
        "    *   Windows: Используйте \"Установка и удаление программ\".\n",
        "    *   Linux: Следуйте инструкциям для вашего метода установки.\n",
        "*   **Удаление моделей Ollama (например, DeepSeek):**\n",
        "    *   Найдите и удалите папку `.ollama/models` в вашей домашней директории (`~/.ollama/models` на macOS/Linux, `C:\\Users\\%username%\\.ollama\\models` на Windows).\n",
        "*   **Удаление векторной базы данных:**\n",
        "    *   Удалите папку, указанную в `persist_directory` (в нашем примере `./chroma_db_rag_concise`).\n",
        "*   **Удаление Python-пакетов:**\n",
        "    *   `pip uninstall ollama langchain chromadb gradio pymupdf langchain-community`\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}